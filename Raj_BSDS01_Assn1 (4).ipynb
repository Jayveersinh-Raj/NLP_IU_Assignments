{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tweets Tokenization\n",
        "\n",
        "The goal of the assignment is to write a tweet tokenizer. The input of the code will be a set of tweet text and the output will be the tokens in each tweet. The assignment is made up of four tasks.\n",
        "\n",
        "The [data](https://drive.google.com/file/d/15x_wPAflvYQ2Xh38iNQGrqUIWLj5l5Nw/view?usp=share_link) contains 5 files whereby each contains 44 tweets. Each tweet is separated by a newline. For manual tokenization only one file should be used.\n",
        "\n",
        "Grading:\n",
        "- 30 points - Tokenize tweets by hand\n",
        "- 30 points - Implement 4 tokenizers\n",
        "- 20 points - Stemming and Lemmatization\n",
        "- 20 points - Explain sentencepiece (for masters only)\n",
        "\n",
        "\n",
        "Remarks: \n",
        "- Use Python 3 or greater\n",
        "- Max is 80 points for bachelors, 100 points for masters"
      ],
      "metadata": {
        "id": "RKboZnAdgrRM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize tweets by hand\n",
        "\n",
        "As a first task you need to tokenize 15 tweets by hand. This will allow you to understand the problem from a linguistic point of view. The guidelines for tweet tokenization are as follows:\n",
        "\n",
        "- Each smiley is a separate token\n",
        "- Each hashtag is an individual token. Each user reference is an individual token\n",
        "- If a word has spaces between them then it is converted to a single token\n",
        "- If a sentence ends with a word that legitimately has a full stop (abbreviations, for example), add a final full stop\n",
        "- All punctuations are individual tokens. This includes double-quotes and single quotes also\n",
        "- A URL is a single token\n",
        "\n",
        "Example of output\n",
        "\n",
        "    Input tweet\n",
        "    @xfranman Old age has made N A T O!\n",
        "\n",
        "    Tokenized tweet (separated by comma)\n",
        "    @xfranman , Old , age , has , made , NATO , !"
      ],
      "metadata": {
        "id": "aLDjjAvemUP_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unzipping the zipped file"
      ],
      "metadata": {
        "id": "zBzQ059ML5KB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip Assignment1_data.zip -d '/content/tweets'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-qJu_1PMV7Y",
        "outputId": "d0626578-2cf4-4faf-d054-df3e6c08582f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Assignment1_data.zip\n",
            "  inflating: /content/tweets/file5   \n",
            "  inflating: /content/tweets/__MACOSX/._file5  \n",
            "  inflating: /content/tweets/file4   \n",
            "  inflating: /content/tweets/__MACOSX/._file4  \n",
            "  inflating: /content/tweets/file3   \n",
            "  inflating: /content/tweets/__MACOSX/._file3  \n",
            "  inflating: /content/tweets/file2   \n",
            "  inflating: /content/tweets/__MACOSX/._file2  \n",
            "  inflating: /content/tweets/file1   \n",
            "  inflating: /content/tweets/__MACOSX/._file1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to open the different tweet files and merge them into one\n",
        "def open_and_merge(filename):\n",
        "\n",
        "  with open('/content/tweets/' + filename) as f:\n",
        "      lines = f.readlines()\n",
        "\n",
        "  with open('/content/merged', 'w') as merged:\n",
        "      merged.writelines(lines)"
      ],
      "metadata": {
        "id": "EsZZMVWKNUiw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  open_and_merge('file' + str(i+1))"
      ],
      "metadata": {
        "id": "8cfDxMVb88Kk"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For the 1st task using only 1st file\n",
        "with open('/content/tweets/file1') as f:\n",
        "      lines = f.readlines()\n",
        "lines[:15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpYQ-rChNdfu",
        "outputId": "e0657163-ca1c-46c8-a394-e0d7d9e4042d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@anitapuspasari waduh..\\n',\n",
              " '\" Could journos please stop putting the word \"\"gate\"\" after everything they write... gate.\"\\n',\n",
              " \"20% More Ridiculous Sale @20x200 ends tonight! - get 20% off by entering 'RIDONK' at checkout. More info: http://bit.ly/ridonktues\\n\",\n",
              " \"@Studio85 I have a pair of those shoes. They are comfy. Like being barefoot. Okay for running, but not on concrete, as I've discovered.\\n\",\n",
              " 'RT @twilightus Team Carlisle is a Trending Topic- help him out RT Follow @peterfacinelli see a grown man n a bikini dance Hollywood Blvd\\n',\n",
              " '@karenrubin you might have to reinstall - that happened to me a few months ago, now I use Nambu on my Mac\\n',\n",
              " 'Just Posted: Redneck Dragon - Part XXVIII (http://cli.gs/gWy0yT)\\n',\n",
              " '\" \"\"Paul McCartney ... went through all his education there and nobody thought he had any musical talent,\"\" http://tinyurl.com/nkdbdq\"\\n',\n",
              " '@ambienteer Yeah, pretty much how i feel about it.\\n',\n",
              " '@florianseroussi Nothing really noticeable? Are you kidding?\\n',\n",
              " '@toiletooth Hours?\\n',\n",
              " '\" Obama,Hamas,and the Mullahs being \"\"helpfu l\"\"http://www.jpost.com/servlet/Satellite?cid=1245184848467&pagename=JPost%2FJPArticle%2FPrinter\"\\n',\n",
              " 'RT @BBHLabs 81% of twitter users are UNDER 30 + more v. interesting statistics here: http://www.sysomos.com/insidetwitter/\\n',\n",
              " \"@Birdingperu Great looking hummer! RTThe world's most spectacular hummingbird Marvelous Spatuletail on a feeder. http://bit.ly/aGHYZ\\n\",\n",
              " 'attn. chas. whitman: RT @villagevoice Jonas Brothers at Rockefeller Center for the Today Show tomorrow morn—EEEEEEEEEE!\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "    1. Input tweet\n",
        "    @anitapuspasari waduh..\\n\n",
        "\n",
        "    1. Tokenized tweet\n",
        "    @anitapuspasari, waduh, .., \\n\n",
        "\n",
        "    2. Input tweet\n",
        "    \" Could journos please stop putting the word \"\"gate\"\" after everything they write... gate.\"\\n\n",
        "    \n",
        "    2. Tokenized tweet\n",
        "    \", Could, journos, please, stop, putting, the, word, \"\", gate, \"\", after, everything, they, write, ..., gate, ., \", \\n\n",
        "\n",
        "    3. Input tweet\n",
        "    20% More Ridiculous Sale @20x200 ends tonight! - get 20% off by entering 'RIDONK' at checkout. More info: http://bit.ly/ridonktues\\n\n",
        "\n",
        "    3. Tokenized tweet\n",
        "    20%, More, Ridiculous Sale, @20x200, ends, tonight, !, -, get, 20%, off, by, entering, 'RIDONK', at, checkout, ., More, info, :, http://bit.ly/ridonktues\\n\n",
        "\n",
        "    4. Input tweet\n",
        "    @Studio85 I have a pair of those shoes. They are comfy. Like being barefoot. Okay for running, but not on concrete, as I've discovered.\\n\n",
        "\n",
        "    4. Tokenized tweet\n",
        "    @Studio85, I, have, a, pair, of, those, shoes, ., They, are, comfy, ., Like, being, barefoot, ., Okay, for, running, , ,but, not, on, concrete, , ,as, I, ', ve, discovered, ., \\n\n",
        "\n",
        "    5. Input tweet\n",
        "    RT @twilightus Team Carlisle is a Trending Topic- help him out RT Follow @peterfacinelli see a grown man n a bikini dance Hollywood Blvd\\n\n",
        "\n",
        "    5. Tokenized tweet\n",
        "    RT, @twilightus, Team, Carlisle, is, a, Trending, Topic, -, help, him, out, RT, Follow, @peterfacinelli, see, a, grown, man, n, a, bikini, dance, Hollywood, Blvd\\n\n",
        "\n",
        "    6. Input tweet\n",
        "    @karenrubin you might have to reinstall - that happened to me a few months ago, now I use Nambu on my Mac\\n\n",
        "\n",
        "    6. Tokenized tweet\n",
        "    @karenrubin, you, might, have, to, reinstall, -, that, happened, to, me, a, few, months, ago, , ,now, I, use, Nambu, on, my, Mac\\n\n",
        "\n",
        "    7. Input tweet\n",
        "    Just Posted: Redneck Dragon - Part XXVIII (http://cli.gs/gWy0yT)\\n\n",
        "\n",
        "    7. Tokenizes tweet\n",
        "    Just, Posted, :, Redneck, Dragon, -, Part, XXVIII, (http://cli.gs/gWy0yT)\\n\n",
        "\n",
        "    8. Input tweet\n",
        "    \" \"\"Paul McCartney ... went through all his education there and nobody thought he had any musical talent,\"\" http://tinyurl.com/nkdbdq\"\\n\n",
        "\n",
        "    8. Tokenized tweet\n",
        "    \", \"\", Paul, McCartney, ..., went, through, all, his, education, there, and, nobody, thought, he, had, any, musical, talent, , , \"\", http://tinyurl.com/nkdbdq, \", \\n\n",
        "\n",
        "    9. Input tweet\n",
        "    @ambienteer Yeah, pretty much how i feel about it.\\n\n",
        "\n",
        "    9. Tokenized tweet\n",
        "    @ambienteer, Yeah, , , pretty, much, how, i, feel, about, it, ., \\n\n",
        "\n",
        "    10. Input tweet\n",
        "    @florianseroussi Nothing really noticeable? Are you kidding?\\n\n",
        "\n",
        "    10. Tokenized tweet\n",
        "    @florianseroussi, Nothing, really, noticeable, ?, Are, you, kidding, ?, \\n\n",
        "\n",
        "    11. Input tweet\n",
        "    @toiletooth Hours?\\n\n",
        "\n",
        "    11. Tokenized tweet\n",
        "    @toiletooth, Hours, ?, \\n\n",
        "\n",
        "    12. Input tweet\n",
        "    \" Obama,Hamas,and the Mullahs being \"\"helpfu l\"\"http://www.jpost.com/servlet/Satellite?cid=1245184848467&pagename=JPost%2FJPArticle%2FPrinter\"\\n\n",
        "\n",
        "    12. Tokenized tweet\n",
        "    \", Obama, , , Hamas, , , and, the, Mullahs, being, \"\", helpfu, l, \"\", http://www.jpost.com/servlet/Satellite?cid=1245184848467&pagename=JPost%2FJPArticle%2FPrinter, \", \\n\n",
        "\n",
        "    13. Input tweet\n",
        "    RT @BBHLabs 81% of twitter users are UNDER 30 + more v. interesting statistics here: http://www.sysomos.com/insidetwitter/\\n\n",
        "\n",
        "    13. Tokenized tweet\n",
        "    RT, @BBHLabs, 81%, of, twitter, users, are, UNDER, 30, +, more, v, ., interesting, statistics, here, :, http://www.sysomos.com/insidetwitter/\\n\n",
        "\n",
        "    14. Input tweet\n",
        "    @Birdingperu Great looking hummer! RTThe world's most spectacular hummingbird Marvelous Spatuletail on a feeder. http://bit.ly/aGHYZ\\n\n",
        "\n",
        "    14. Tokenized tweet\n",
        "    @Birdingperu, Great, looking, hummer, !, RTThe, world's, most, spectacular, hummingbird, Marvelous, Spatuletail, on, a, feeder, ., http://bit.ly/aGHYZ\\n\n",
        "\n",
        "    15. Input tweet\n",
        "    attn. chas. whitman: RT @villagevoice Jonas Brothers at Rockefeller Center for the Today Show tomorrow morn—EEEEEEEEEE!\\n\n",
        "\n",
        "    15. Input tweet\n",
        "    attn, ., chas, ., whitman, :, RT, @villagevoice, Jonas, Brothers, at, Rockefeller, Center, for, the, Today, Show, tomorrow, morn, —, EEEEEEEEEE, !, \\n\n"
      ],
      "metadata": {
        "id": "7KKKwTidnzUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement 4 tokenizers\n",
        "\n",
        "Your task is to implement the 4 different tokenizers that take a list of tweets on a topic and output tokenization for each:\n",
        "\n",
        "- White Space Tokenization\n",
        "- Sentencepiece\n",
        "- Tokenizing text using regular expressions\n",
        "- NLTK TweetTokenizer\n",
        "\n",
        "For tokenizing text using regular expressions use the rules in task 1. Combine task 1 rules into regular expression and create a tokenizer."
      ],
      "metadata": {
        "id": "-2J2AD2nmUhi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading merged tweets"
      ],
      "metadata": {
        "id": "w2mURRmaKb5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving all the tweets in tweets\n",
        "with open('/content/merged') as f:\n",
        "      tweets = f.readlines()"
      ],
      "metadata": {
        "id": "c3dsv-XV-6iK"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. WhiteSpace tokenizer"
      ],
      "metadata": {
        "id": "mWlOcBltKefm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk whitespace tokenizer\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "# Function for whitespace tokenization\n",
        "def white_space_tokenizer(text: str) -> list:\n",
        "    tk = WhitespaceTokenizer()\n",
        "    return tk.tokenize(text)"
      ],
      "metadata": {
        "id": "4uZId1tjrfyz"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The following is an example on 1 file, all the files can also be used"
      ],
      "metadata": {
        "id": "dSS_CEy07670"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = []\n",
        "for tweet in tweets:\n",
        "   tokens.append(white_space_tokenizer(tweet))\n",
        "tokens[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLvyuqBx46aO",
        "outputId": "330ef685-a22c-4981-b791-3b666e9dbdf5"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Being',\n",
              " 'A',\n",
              " 'Work',\n",
              " 'At',\n",
              " 'Home',\n",
              " 'Mom',\n",
              " '(WAHM)',\n",
              " 'Is',\n",
              " 'A',\n",
              " '24/7',\n",
              " 'Job',\n",
              " '»',\n",
              " 'Messing',\n",
              " 'With',\n",
              " 'My',\n",
              " 'Mind',\n",
              " 'http://bit.ly/17rLra']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Sentence piece tokenizer"
      ],
      "metadata": {
        "id": "8fSVp9QDKqIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXj9Q4hj67CA",
        "outputId": "f279377a-41bd-4b68-ef47-37770f744ac3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# Training on the tweets model for tokenization\n",
        "spm.SentencePieceTrainer.Train(input='/content/merged',\n",
        "                               model_prefix='sp_tweets',\n",
        "                               vocab_size=520,\n",
        "                               pad_id=0,                \n",
        "                               unk_id=1,\n",
        "                               bos_id=2,\n",
        "                               eos_id=3\n",
        "                               )"
      ],
      "metadata": {
        "id": "4ou1WE8Krf_W"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for sentence piece tokenizer\n",
        "def sentencepiece_wrapper(text: str) -> list:\n",
        "   \n",
        "   sp = spm.SentencePieceProcessor(model_file=str('/content/sp_tweets.model'))\n",
        "   encoded_input = sp.Encode(text)\n",
        "\n",
        "   tokenized_input = [sp.IdToPiece(id) for id in encoded_input]\n",
        "    \n",
        "   return tokenized_input"
      ],
      "metadata": {
        "id": "ePbCRH2yAtry"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp_tokens = []\n",
        "for tweet in tweets:\n",
        "   sp_tokens.append(sentencepiece_wrapper(tweet))\n",
        "sp_tokens[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmpdsR-pAAHu",
        "outputId": "e3f3014f-805b-48ff-f1e8-cbda92717271"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁Be',\n",
              " 'ing',\n",
              " '▁A',\n",
              " '▁W',\n",
              " 'or',\n",
              " 'k',\n",
              " '▁A',\n",
              " 't',\n",
              " '▁H',\n",
              " 'ome',\n",
              " '▁Mo',\n",
              " 'm',\n",
              " '▁(',\n",
              " 'W',\n",
              " 'A',\n",
              " 'H',\n",
              " 'M',\n",
              " ')',\n",
              " '▁Is',\n",
              " '▁A',\n",
              " '▁2',\n",
              " '4',\n",
              " '/',\n",
              " '7',\n",
              " '▁J',\n",
              " 'o',\n",
              " 'b',\n",
              " '▁',\n",
              " '»',\n",
              " '▁M',\n",
              " 'ess',\n",
              " 'ing',\n",
              " '▁W',\n",
              " 'i',\n",
              " 'th',\n",
              " '▁M',\n",
              " 'y',\n",
              " '▁M',\n",
              " 'in',\n",
              " 'd',\n",
              " '▁h',\n",
              " 't',\n",
              " 'tp',\n",
              " '://',\n",
              " 'bi',\n",
              " 't',\n",
              " '.',\n",
              " 'ly',\n",
              " '/',\n",
              " '17',\n",
              " 'r',\n",
              " 'L',\n",
              " 'ra']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Regex tokenization"
      ],
      "metadata": {
        "id": "vUk5KidqKuSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Function for tokenization using regex \n",
        "def re_tokenizer(text: str) -> list:\n",
        "    token = re.split('[^\\S\\\"\\'\\-\\:\\,\\;\\.\\!\\?]+', text)\n",
        "    return token"
      ],
      "metadata": {
        "id": "pC32dK_urf5P"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re_tokens = []\n",
        "for tweet in tweets:\n",
        "   re_tokens.append(re_tokenizer(tweet))\n",
        "re_tokens[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_3pD-EDDO8N",
        "outputId": "81ac952f-bef0-404a-fcb0-b95dfbc32d5d"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Being',\n",
              " 'A',\n",
              " 'Work',\n",
              " 'At',\n",
              " 'Home',\n",
              " 'Mom',\n",
              " '(WAHM)',\n",
              " 'Is',\n",
              " 'A',\n",
              " '24/7',\n",
              " 'Job',\n",
              " '»',\n",
              " 'Messing',\n",
              " 'With',\n",
              " 'My',\n",
              " 'Mind',\n",
              " 'http://bit.ly/17rLra',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Nltk tweet tokenizer"
      ],
      "metadata": {
        "id": "vgNheTy3K3kS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "# Function for nltk tweet tokenization\n",
        "def nltk_tweet_tokenizer(text: str) -> list:\n",
        "    tk = TweetTokenizer()\n",
        "    return tk.tokenize(text)"
      ],
      "metadata": {
        "id": "Q8UVniWVrgMT"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_tokens = []\n",
        "for tweet in tweets:\n",
        "   nltk_tokens.append(nltk_tweet_tokenizer(tweet))\n",
        "nltk_tokens[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcWC0va7GEHG",
        "outputId": "6b5b5258-ee05-4e81-d30a-1f34744d9d19"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Being',\n",
              " 'A',\n",
              " 'Work',\n",
              " 'At',\n",
              " 'Home',\n",
              " 'Mom',\n",
              " '(',\n",
              " 'WAHM',\n",
              " ')',\n",
              " 'Is',\n",
              " 'A',\n",
              " '24/7',\n",
              " 'Job',\n",
              " '»',\n",
              " 'Messing',\n",
              " 'With',\n",
              " 'My',\n",
              " 'Mind',\n",
              " 'http://bit.ly/17rLra']"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run your implementations on the data. Compare the results, decide which one is better. List the advantages of the best tokenizer."
      ],
      "metadata": {
        "id": "yIhPteb4s_Yn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   All of them are good, but depending on the use case and domain of the problem itself. If something like a sentiment or disaster or any type of classificiation required on a supervised set of such data then nltk, regex, whitespace all would work\n",
        "*   Sentence piece is the best one because it dissects the words themselves which makes the usage more broad and powerful, especially in cross lingual and or multi-lingual applications.\n",
        "\n"
      ],
      "metadata": {
        "id": "muAZeHkMtaCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming and Lemmatization\n",
        "\n",
        "Your task is to write two functions: stem and lemmatize. Input is a text, so you need to tokenize it first."
      ],
      "metadata": {
        "id": "MlzpzjgEpY-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The nltk tweet tokenized tokens would be used for this task"
      ],
      "metadata": {
        "id": "p0Db56KkHnD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "def stem(text: str) -> list:\n",
        "    snow_stemmer = SnowballStemmer(language='english')\n",
        "    \n",
        "    # To store the stem words\n",
        "    stem_words = []\n",
        "    for word in text:\n",
        "      x = snow_stemmer.stem(word)\n",
        "      stem_words.append(x)\n",
        "    return stem_words"
      ],
      "metadata": {
        "id": "ghAR1rSjsnz1"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing on nltk tweet tokenizer tokens"
      ],
      "metadata": {
        "id": "pNRotVX-IBEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stem_words = []\n",
        "for token in nltk_tokens:\n",
        "  stem_words.append(stem(token))\n",
        "stem_words[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JYOYA4lIAQs",
        "outputId": "deb06bb7-e96e-4d0a-bf45-3b5513e63ae5"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['be',\n",
              " 'a',\n",
              " 'work',\n",
              " 'at',\n",
              " 'home',\n",
              " 'mom',\n",
              " '(',\n",
              " 'wahm',\n",
              " ')',\n",
              " 'is',\n",
              " 'a',\n",
              " '24/7',\n",
              " 'job',\n",
              " '»',\n",
              " 'mess',\n",
              " 'with',\n",
              " 'my',\n",
              " 'mind',\n",
              " 'http://bit.ly/17rlra']"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization"
      ],
      "metadata": {
        "id": "LxxuBDwCLA0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOK1LCeHJFRF",
        "outputId": "0d786f40-da50-4e09-a891-9fd6cc46d4a7"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-02-14 13:36:47.136014: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-14 13:36:47.136107: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-14 13:36:47.136127: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-02-14 13:36:48.392833: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-md==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.4.1/en_core_web_md-3.4.1-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-md==3.4.1) (3.4.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (23.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.12)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.25.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.10.4)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.0.4)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (8.1.7)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.1)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.4.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "# Function for lemmatization using spacy\n",
        "def lemmatize(text: str) -> list:\n",
        "    lemma_tokens = []\n",
        "    tokens = nlp(text)\n",
        "    for token in tokens:\n",
        "      lemma_tokens.append(token.lemma_)\n",
        "    return lemma_tokens"
      ],
      "metadata": {
        "id": "kZkR3TPYuk5T"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemma_words = []\n",
        "for tweet in tweets:\n",
        "  lemma_words.append(lemmatize(tweet))\n",
        "lemma_words[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJfw6pa4JLWQ",
        "outputId": "5a8aae38-65f9-4096-eb74-ac99899be9e5"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['be',\n",
              " 'a',\n",
              " 'work',\n",
              " 'at',\n",
              " 'home',\n",
              " 'Mom',\n",
              " '(',\n",
              " 'WAHM',\n",
              " ')',\n",
              " 'be',\n",
              " 'A',\n",
              " '24/7',\n",
              " 'job',\n",
              " '»',\n",
              " 'mess',\n",
              " 'with',\n",
              " 'my',\n",
              " 'mind',\n",
              " 'http://bit.ly/17rlra',\n",
              " '\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explain sentencepiece (for masters only)\n",
        "\n",
        "For this task you will have to use sentencepiece text tokenizer. Your task will be to read how it works and write a minimum 10 sentences explanation of the tokenizer works."
      ],
      "metadata": {
        "id": "islrHZ6UmUoh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "e2RjMwlEshCu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resources\n",
        "\n",
        "1. [Regular Expressions 1](https://realpython.com/regex-python/)\n",
        "2. [Regular Expressions 2](https://realpython.com/regex-python-part-2/)\n",
        "2. [Spacy Lemmatizer](https://spacy.io/api/lemmatizer)\n",
        "2. [NLTK Stem](https://www.nltk.org/howto/stem.html)\n",
        "3. [SentencePiece](https://github.com/google/sentencepiece)\n",
        "4. [sentencepiece tokenizer](https://towardsdatascience.com/sentencepiece-tokenizer-demystified-d0a3aac19b15)"
      ],
      "metadata": {
        "id": "cmNpWzfLmUyE"
      }
    }
  ]
}