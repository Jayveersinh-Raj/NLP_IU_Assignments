{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Jayveersinh Raj\n",
        "# BS-DS-01\n",
        "# j.raj@innopolis.university"
      ],
      "metadata": {
        "id": "W4cXE8e2PRT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 40 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n",
        "- 20 points - Evaluate on Github Typo Corpus (for masters only)\n",
        "\n",
        "\n",
        "Remarks: \n",
        "- Use Python 3 or greater\n",
        "- Max is 80 points for bachelors, 100 points for masters"
      ],
      "metadata": {
        "id": "DIgM6C9HYUhm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html).\n",
        "\n",
        "[N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf)\n",
        "\n",
        "You may also wnat to implement:\n",
        "- spell-checking for a concrete language - Russian, Tatar, Ukranian, etc. - any one you know, such that the solution accounts for language specifics,\n",
        "- some recent (or not very recent) paper on this topic,\n",
        "- solution which takes into account keyboard layout and associated misspellings,\n",
        "- efficiency improvement to make the solution faster,\n",
        "- any other idea of yours to improve the Norvigâ€™s solution.\n",
        "\n",
        "Important - your project should not be a mere code copy-paste from somewhere. Implement yourself, analyze why it was suggested this way, and think of improvements/customization.\n",
        "\n",
        "Your solution should be able to perform 4 corrections per second (3-5 words in an example) on a typical cpu."
      ],
      "metadata": {
        "id": "x-vb8yFOGRDF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Justify your decisions\n",
        "In your implementation you will need to decide which ngram dataset to use, which weights to assign for edit1, edit2 or absent words probabilities, beam search parameters and etc. Write down justificaitons for these choices."
      ],
      "metadata": {
        "id": "oML-5sJwGRLE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spell checker"
      ],
      "metadata": {
        "id": "17Xq6UpaG09E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The read bigrams function to read the bigrams\n",
        "def read_file(file_path):\n",
        "    freq_table = {}\n",
        "    with open(file_path, 'r', encoding='ISO-8859-1') as f:\n",
        "        for line in f:         \n",
        "            freq, *bigrams = line.strip().split('\\t') \n",
        "            bigrams = ' '.join(bigrams)\n",
        "            freq_table[bigrams.lower()] = int(freq)\n",
        "    return freq_table\n",
        "\n",
        "BIGRAMS = read_file('/content/w2_.txt')\n",
        "\n",
        "# Total frequency\n",
        "N=sum(BIGRAMS.values())\n",
        "\n",
        "def P(bigram):\n",
        "    \"Probability of `bigram`.\"\n",
        "    return BIGRAMS.get(bigram, 0) / N\n",
        "\n",
        "def edits1(word):\n",
        "    '''\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    word : word\n",
        "    return : a collection of all editable strings (words and non-words alike) that can be created with a single edit\n",
        "    '''\n",
        "\n",
        "     # Define ASCII codes for lowercase letters\n",
        "    alphabets = range(97, 123)\n",
        "\n",
        "    # Split the word into left and right substrings\n",
        "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "\n",
        "    # Remove one letter from the word\n",
        "    deletes = [left + right[1:] for left, right in splits if right]\n",
        "\n",
        "    # Transpose adjacent letters\n",
        "    transposes = [left + right[1] + right[0] + right[2:] for left, right in splits if len(right) > 1]\n",
        "\n",
        "    # Replace a letter with another letter from alphabets\n",
        "    replaces = [left + chr(c) + right[1:] for left, right in splits if right for c in alphabets if c != ord(right[0])]\n",
        "\n",
        "    # Insert a letter from alphabets\n",
        "    inserts = [left + chr(c) + right for left, right in splits for c in alphabets]\n",
        "\n",
        "    # Return all possible words as a set\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "\n",
        "def edits2(word): \n",
        "    '''\n",
        "    All edits that are two edits away from `word`.\n",
        "    word : word\n",
        "    return : a collection of all editable strings (words and non-words alike) that can be created with a two edit\n",
        "    '''\n",
        "\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "\n",
        "def existing(words): \n",
        "    '''\n",
        "    The subset of `words` that appear in the dictionary of WORDS.\n",
        "    word : word\n",
        "    return : a set of all existing words in the dictionary\n",
        "    '''\n",
        "\n",
        "    return set(w for w in words if w in BIGRAMS.keys())\n",
        "\n",
        "def candidates(word): \n",
        "    '''\n",
        "    Generate possible spelling corrections for word.\n",
        "    word : word\n",
        "    return : a collection of viable candidates\n",
        "    '''\n",
        "    return (existing([word]) or existing(edits1(word)) or existing(edits2(word)) or [word])\n",
        "\n",
        "def correction(word): \n",
        "    '''\n",
        "    Most probable spelling correction for word.\n",
        "    word: word\n",
        "    returns the most likely\n",
        "    '''\n",
        "    return max(candidates(word), key=P)"
      ],
      "metadata": {
        "id": "fem57l8aMSTw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing on a word by calling `correction function`"
      ],
      "metadata": {
        "id": "puisaXleG6rL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correction('a cicle')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SFKhAqetMd1z",
        "outputId": "8cde498d-d63d-4b90-beeb-a6834e6fc671"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a circle'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Justification\n",
        "### 1. Choosing bi-gram over n-gram with n=5 : \n",
        "An n-gram is a continuous series of n elements from the text, whereas a bigram is a pair of adjacent words in a passage (e.g., letters, words, or even characters). Bigrams and n-grams are used in spell checkers to determine how to spell a word correctly based on the context in which it appears.\n",
        "\n",
        "Bigrams are typically preferred for spell checking over n-grams with high n values (such as n=5) for a number of reasons:\n",
        "- `Smaller Context`: Bigrams give the spelling correction algorithm a more focused context. Using a bigram, the algorithm simply needs to take into account the target word's two closest neighbors. It is frequently possible to determine the right spelling from this context. However, n-grams with a large n number may supply too much context, making it more challenging to extract the important details.\n",
        "- `Improved Handling of Word Variations`: The algorithm is better able to manage word usage changes when using bigrams. For example, if the target word is \"their\", the bigram \"their house\" provides more relevant context than a 5-gram that includes numerous words between \"their\" and \"house\". Despite the fact that \"their\" might not be the most popular option in that location, this can help the algorithm identify that it is right.\n",
        "\n",
        "- `Computationally Efficient`: Bigrams are typically computed more quickly and with fewer resources than n-grams with a high value of n. This is because the algorithm only needs to look at a little bit of information for each bigram, which results in fewer bigrams to take into account.\n",
        "\n",
        "Generally, `n-grams` can offer `more context` than `bigrams`, but they can also be `harder` to deal with and `more expensive to compute`. `Bigrams` provide a `reasonable balance` between `context and efficiency` for many `applications`, including `spell checking`.\n",
        "\n",
        "### 2. The core spell checking algorithm:\n",
        "It is very similar to `Norvig's Solution`.\n",
        "- `The probability function`: It is simply `getting it from dictionary`, and dividing its `frequency` by `total frequencies`. It could be done `more complex`, and `even better`, but it would create more `complexity`. For example `Hamming distance` and `Levenshtein distance` could be used. But `Frequency table` is already given which `eases` the `complexity`.\n",
        "\n",
        "- `The edits function`: They are same as Norvig's solution, the reason is that in order to `fairly compare` both uni-gram (i.e. Norvig's solution), and bi-gram as I created, they must be the same. Moreover, the point down to this would explain it even better\n",
        "\n",
        "- `Possible customizations considered`: \n",
        "1. Swapping vowels: Adds an extra layer of complexity, and would not help much\n",
        "\n",
        "2. Using `AI` based `algorithms` like `Genetic Algorithm`: Computationally very heavy, best solution to provide approximate better solutions using a very efficient `fitting function`, but `computationally inefficient`, `memory inefficient`\n",
        "\n",
        "3. `Conclusion`: All in all there exists better solutions, but for this one to be in parallel with Norvig's solution or to compare it with his solution, it should be fair"
      ],
      "metadata": {
        "id": "vTig0gm6HKAB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
      ],
      "metadata": {
        "id": "46rk65S4GRSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating a small test set to compare with Norvig's solution"
      ],
      "metadata": {
        "id": "qplsE2XTOXj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a test set \n",
        "test_lines = [\n",
        "\"a bicyle: a bycyle\",\n",
        "\"a circle: a cicle\",\n",
        "\"an umbrella: an umbrlela\",\n",
        "\"a baby: a babi\",\n",
        "\"during sports: dring sports\",\n",
        "\"as above: as aboev\",\n",
        "\"a another: a anoter\",\n",
        "\"dying species: dying species\",\n",
        "\"during lunch: dring lunch\",\n",
        "]\n",
        "\n",
        "# Writing to a test_data.txt file\n",
        "with open(\"test_data.txt\", \"w\") as f:\n",
        "  for pairs in test_lines:\n",
        "    f.write(f\"{pairs}\\n\")"
      ],
      "metadata": {
        "id": "cB8Jlv1GFY-v"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation function"
      ],
      "metadata": {
        "id": "4culrw_MOgN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def test_bigram_spellchecker():\n",
        "    '''Test the bigram-based spell checker on the above generated test data'''\n",
        "    from collections import Counter\n",
        "    \n",
        "    # load test data\n",
        "    with open('/content/test_data.txt') as f:\n",
        "        test_data = f.read().splitlines()\n",
        "    \n",
        "    # storing total number of test set\n",
        "    n = len(test_data)\n",
        "\n",
        "    # start timer\n",
        "    start = time.process_time()\n",
        "    \n",
        "    # iterate over test data and evaluate spell checker performance\n",
        "    total_count, correct_count = 0, 0\n",
        "    for line in test_data:\n",
        "        words = line.split(':')\n",
        "        correct_word = words[0]\n",
        "        misspelled_words = words[1:]\n",
        "        \n",
        "        # get the prediction on the misspelled word\n",
        "        for word in misspelled_words:\n",
        "           suggestion = correction(word)\n",
        "           if suggestion == correct_word:\n",
        "             correct_count += 1\n",
        "           total_count += 1\n",
        "                    \n",
        "    dt = time.process_time() - start\n",
        "    # print results\n",
        "    accuracy = correct_count / total_count\n",
        "    print(f'Test results: {correct_count}/{total_count} correct ({accuracy:.2%} accuracy), with {int(n/dt)} words per second')"
      ],
      "metadata": {
        "id": "OwZWaX9VVs7B"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calling the evaluation function to compare"
      ],
      "metadata": {
        "id": "HrP00Y5TOk-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_bigram_spellchecker()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-p0YjUx9Vpp",
        "outputId": "ffa19ce9-1779-4579-fcd8-8569350ab9e7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test results: 5/9 correct (55.56% accuracy), with 4 words per second\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Comparison"
      ],
      "metadata": {
        "id": "MXLudZ7MOrTI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The above implemented bi-grams solution : `~56%`\n",
        "## Norvig's solution : `68%`"
      ],
      "metadata": {
        "id": "b3egjrsUOy3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate on Github Typo Corpus\n",
        "Now, you need to evaluate your solution on the Github Typo Corpus. Don't forget to compare the accuracy with the Norvig's solution."
      ],
      "metadata": {
        "id": "VISJtkFD4VhV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP9GZCYsWjgB"
      },
      "outputs": [],
      "source": [
        "!wget https://github-typo-corpus.s3.amazonaws.com/data/github-typo-corpus.v1.0.0.jsonl.gz\n",
        "!gzip -d github-typo-corpus.v1.0.0.jsonl.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jsonlines\n",
        "\n",
        "dataset_file = \"github-typo-corpus.v1.0.0.jsonl\"\n",
        "\n",
        "dataset = []\n",
        "other_langs = set()\n",
        "\n",
        "with jsonlines.open(dataset_file) as reader:\n",
        "    for obj in reader:\n",
        "        for edit in obj['edits']:\n",
        "            if edit['src']['lang'] == 'eng' and edit['is_typo']:\n",
        "                src, tgt = edit['src']['text'], edit['tgt']['text']\n",
        "                if src.lower() != tgt.lower():\n",
        "                    dataset.append((src, tgt))\n",
        "                \n",
        "print(f\"Dataset size = {len(dataset)}\")"
      ],
      "metadata": {
        "id": "oiUTzkLNGr2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please, explore the dataset. You may see, that this is\n",
        "- mostly markdown\n",
        "- some common mistakes with do/does\n",
        "- some just refer to punctuation typos (which we do not consider)"
      ],
      "metadata": {
        "id": "hjiKaUj0HKYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for pair in dataset[1010:1020]:\n",
        "    print(f\"{pair[0]} => {pair[1]}\")"
      ],
      "metadata": {
        "id": "gpkAqj6RHOr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare your implementation with Norvig's solution on this dataset"
      ],
      "metadata": {
        "id": "8p0kp4G-HexP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "limit = 10000\n",
        "counter = limit\n",
        "for i, (src, target) in enumerate(dataset):\n",
        "    if i == limit:\n",
        "        break\n",
        "    # YOUR CODE HERE"
      ],
      "metadata": {
        "id": "vy85_6oKHE3b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}